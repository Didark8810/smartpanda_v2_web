-- Script para insertar datos de ML Optimization en tabla Nota
INSERT INTO Nota (
    titulo, 
    detalle, 
    ayuda, 
    fecha, 
    libre, 
    idTema, 
    idSubtema, 
    idUsuario, 
    repaso, 
    favorito, 
    importancia
)
VALUES
    ('Optimization in ML', 'The mathematical process of adjusting model parameters to minimize a specific objective function, known as the loss function, which is how models learn from data. It is the core of the learning process itself, not a post-processing step.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Loss Function', 'Quantifies the model’s prediction error during training. It measures how far predictions are from the true values and defines the objective to minimize. Examples include Mean Squared Error (MSE) and Cross-Entropy Loss.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Model Parameters', 'The internal values (like weights and biases) that the model adjusts during training. These are what optimization seeks to improve.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Optimizer', 'The algorithm that updates model parameters based on the loss. Examples include Gradient Descent, Adam, and RMSProp.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Training Data', 'The source of feedback for optimization, comprising input features (X) and true labels (Y).', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Inference', 'Happens during testing/deployment, where the learned parameters are used to make predictions. It is enabled by the optimization phase.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Non-Convergence', 'An optimization failure where the optimizer fails to reduce the loss meaningfully. This can be caused by a poor learning rate, bad initialization, or vanishing/exploding gradients.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Local Minima or Saddle Points', 'Optimization failures where the model gets stuck in suboptimal solutions with flat gradients.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Overfitting', 'An optimization failure where the model over-learns the training data, leading to poor generalization to new data.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Misaligned Objectives', 'An optimization failure where the optimization process minimizes the wrong metric for the real-world task. This often stems from poor problem framing.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Gradient', 'A mathematical tool that tells us which direction to move in order to reduce a function’s value the fastest. In 1D, it''s the slope of the function. In optimization, we use the negative gradient to update model parameters and reduce the loss.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Auto-Differentiation', 'A technique used by modern libraries like PyTorch and TensorFlow to automatically compute gradients.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Gradient Descent Update Rule', 'θ ← θ −η ·∇θ L(θ), where θ represents model parameters, η is the learning rate, and ∇θ L(θ) is the gradient of the loss with respect to the parameters. The key idea is to move in the direction that decreases the loss.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Batch Gradient Descent (BGD)', 'A type of gradient descent where parameter updates are made using the entire dataset at once. It is very stable with smooth convergence but can be slow and memory-intensive, especially for large datasets.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Stochastic Gradient Descent (SGD)', 'An optimization technique that updates model parameters incrementally using a single or mini-batch sample from the dataset rather than the full dataset. It is very fast and scalable to large data but can be unstable without tuning or momentum.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Mini-Batch Gradient Descent', 'A type of gradient descent that computes the gradient using a mini-batch of m samples, where m is much smaller than the total dataset size N. It offers a balance between stability and performance, being more stable than SGD and faster than Batch GD, and is ideal for deep learning.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Learning Rate (η)', 'A hyperparameter that controls how big a step we take in the direction of the negative gradient during parameter updates. A large learning rate can help escape flat regions, while smaller rates improve convergence near minima.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Learning Rate Decay', 'Strategies to reduce the learning rate over time. Examples include Exponential Decay (ηt = η0 · γ^t) and Inverse Time Decay (ηt = η0 / (1+kt)).', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Momentum', 'A technique that stores past gradients to build velocity. It helps reduce oscillations and speeds convergence, especially in narrow or curved regions of the loss surface. The rule is vt+1 = µvt −η∇L and θt+1 = θt +vt+1.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Nesterov Accelerated Gradient (NAG)', 'An improvement over classical momentum that computes the gradient after moving partway in the direction of momentum, helping to avoid overshooting and stabilize better.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Adam Optimizer', 'An optimizer that combines the benefits of SGD with momentum and adaptive learning rates. It offers fast convergence and adaptive per-parameter updates but may generalize worse than SGD in some cases.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('RMSProp Optimizer', 'An adaptive learning rate optimization algorithm that maintains a moving average of the squared gradients and divides the current gradient by the root of this average. It is useful for handling noisy, non-stationary gradients.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Hyperparameter Tuning', 'The process of finding the best performing hyperparameters for machine learning models.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Grid Search', 'A brute-force hyperparameter tuning approach where all combinations of hyperparameter values are systematically tested. It is simple and reproducible but computationally expensive, as the total configurations grow exponentially with the number of hyperparameters.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Random Search', 'A hyperparameter tuning principle that samples configurations from user-defined distributions. It is efficient, scalable, and covers more individual values than grid search but may miss good regions.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('K-Fold Cross-Validation', 'A validation strategy that splits data into k equal parts, training on k-1 and validating on the remainder. It is used for stable performance estimates, especially in low data regimes.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Nested Cross-Validation', 'A validation strategy that wraps two cross-validation loops (an inner loop for hyperparameter tuning and an outer loop for model evaluation) to prevent overfitting to validation folds during tuning.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Search Space Design', 'The process of defining and controlling which hyperparameters an optimization framework tries. Proper design is crucial for efficiency and finding good models.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Log-Scale Parameters', 'Parameters like learning rate, L2 regularization, or dropout rate that should be sampled from an exponent space (e.g., using suggest_loguniform in Optuna) because their impact is often exponential.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Categorical Parameters', 'Discrete choices like optimizers (e.g., Adam, SGD, RMSProp) where no interpolation is meaningful.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Optuna', 'A modern hyperparameter optimization framework designed for dynamic and adaptive tuning of hyperparameters. It helps avoid wasteful evaluations by pruning bad trials early.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Study (Optuna)', 'In Optuna, a single optimization run that manages the optimization loop over many trials.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Trial (Optuna)', 'In Optuna, one evaluation of the objective function with a candidate set of hyperparameters.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Sampler (Optuna)', 'In Optuna, a component that suggests the next parameters to try.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Pruner (Optuna)', 'In Optuna, a component that stops unpromising trials early, avoiding wasteful evaluations.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Objective Function (Optuna)', 'A function in Optuna that trains and evaluates a model and returns a scalar metric (e.g., validation accuracy to maximize, or loss to minimize).', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Multivariate Normal Distribution (MVN)', 'A foundational mathematical concept for Gaussian Processes, where each random variable is distributed normally and their joint distribution is also Gaussian. It is defined by a mean vector (µ) and a covariance matrix (Σ).', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Covariance Matrix (MVN/GP)', 'In MVNs and GPs, it describes the shape of the distribution. It encodes the relationships (covariances) between different random variables or function evaluations.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Gaussian Process (GP)', 'A distribution over functions, such that any finite set of function values is jointly normally distributed. It is used in Bayesian optimization to model an unknown function.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Mean Function (GP)', 'In a GP, m(x) = E[f(x)], representing the expected value of the function. Often set to 0 for simplicity.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Kernel Function (GP)', 'Also called the covariance function, k(x,x′) defines the covariance between the outputs f(x) and f(x′) of a Gaussian Process. It quantifies the similarity between different inputs and governs smoothness, periodicity, amplitude, and other properties of the modeled functions.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Prior Distribution (GP)', 'In Bayesian inference, it represents the prior belief over the function f(x) before observing any training data.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Bayesian Optimization (BO)', 'A global optimization strategy for expensive black-box functions that are not assumed to be known analytically or differentiable. It uses a probabilistic surrogate model to decide where to evaluate next, balancing exploration and exploitation.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Surrogate Model (BO)', 'In Bayesian Optimization, a probabilistic model (typically a Gaussian Process) that approximates the expensive black-box function f(x) using known evaluations. It predicts both the mean (µ(x)) and uncertainty (σ²(x)) of the function at a given point.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Acquisition Function (BO)', 'In Bayesian Optimization, a function that quantifies the value of evaluating the true function f at a new point x, based on the current surrogate model. It guides the selection of the next point to query.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Probability of Improvement (PI)', 'An acquisition function that chooses the next query point as the one which has the highest probability of improving over the current maximum observed function value.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Upper Confidence Bound (UCB)', 'An acquisition function that combines the mean (µ(x)) and uncertainty (σ(x)) of the surrogate model to balance exploitation (high mean) and exploration (high uncertainty). The hyperparameter λ controls this preference.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('RBF (Squared Exponential) Kernel', 'A common kernel that models smooth functions where similarity decays with distance. Parameters include σ² (overall variance) and ℓ (length scale). It is a stationary kernel.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Linear Kernel', 'A kernel that models functions with linear trends, useful when the function increases steadily. It is a non-stationary kernel.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0),
    ('Periodic Kernel', 'A kernel that captures repeating patterns in functions, using parameters like period p and lengthscale ℓ. It is a stationary kernel.', '', GETDATE(), 0, 28, 43, 1, 0, 0, 0);

-- Mensaje de confirmación
PRINT 'Se insertaron ' + CAST(@@ROWCOUNT AS VARCHAR) + ' registros en la tabla Nota';
